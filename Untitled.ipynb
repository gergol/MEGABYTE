{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83f1b60-5e33-4034-92e0-13e2d1a2ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MEGABYTE_pytorch import MEGABYTE\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# constants\n",
    "\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 3\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "PRIME_LEN = 100\n",
    "SEQ_LEN = 8192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6732c601-f247-4af6-918e-eda6de6f6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"./data/enwik8.gz\") as file:\n",
    "    x = np.frombuffer(file.read(int(95e6)), dtype=np.uint8).copy()\n",
    "    train_x, valid_x = np.split(x, [int(90e6)])\n",
    "    data_train, data_val = map(torch.from_numpy, (train_x, valid_x))\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return \"\".join(list(map(decode_token, tokens)))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start : rand_start + self.seq_len].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "val_dataset = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "train_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\n",
    "val_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c48cda-45ba-448b-81ae-597cde241684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n",
      "Model size: 33.8M parameters\n"
     ]
    }
   ],
   "source": [
    "#model = MEGABYTE(\n",
    "#    num_tokens = 256,\n",
    "#    dim = (768, 512, 256),\n",
    "#    depth = (6, 4, 2),\n",
    "#    max_seq_len = (512, 4, 4),\n",
    "#    flash_attn = True\n",
    "#).cuda()\n",
    "\n",
    "\n",
    "model = MEGABYTE(\n",
    "    vocab_size = 256,             # number of tokens\n",
    "    hidden_sizes = (512, 256),               # transformer model dimension (512 for coarsest, 256 for fine in this example)\n",
    "    max_sequence_lengths = (1024, 16),        # sequence length for global and then local. this can be more than 2\n",
    "    num_hidden_layers = (6, 4),                 # number of layers for global and then local. this can be more than 2, but length must match the max_seq_len's\n",
    "    dim_head = 64,                  # dimension per head\n",
    "    num_heads = 8,                      # number of attention heads\n",
    "    flash_attn = True,              # use flash attention\n",
    "    add_cross_attention=True\n",
    ").cuda()\n",
    "\n",
    "x = torch.randint(0, 16000, (1, 1024, 4))\n",
    "\n",
    "from omr.utils import print_model_size\n",
    "\n",
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7b476e-4402-41c1-9ca5-85684175310b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 8192]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_loader)\n",
    "list(x for x in batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38bfa3a-855b-45c4-bc20-5ea1707ae216",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.rand(BATCH_SIZE, 1201, 512).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f32555c-206f-43fb-9257-8fe97c8660de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "_ = model(batch, encoder_hidden_states=encoder_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ee5fca-550f-4b4c-b611-be193600b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect_shapes MEGABYTE: ['ids=(3, 8192)']\n",
      "inspect_shapes Transformer: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1025, 64)', 'v=(3, 8, 1025, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes before attend: : ['q=(3, 8, 1025, 64)', 'k=(3, 8, 1201, 64)', 'v=(3, 8, 1201, 64)']\n",
      "inspect_shapes Transformer post cross attn: ['x=(3, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(3, 1025, 512)']\n",
      "inspect_shapes MEGABYTE post proj: ['x=(3072, 8, 256)']\n",
      "inspect_shapes Transformer: ['x=(3072, 9, 256)']\n",
      "inspect_shapes before attend: : ['q=(3072, 8, 9, 64)', 'k=(3072, 8, 9, 64)', 'v=(3072, 8, 9, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3072, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(3072, 9, 256)']\n",
      "inspect_shapes before attend: : ['q=(3072, 8, 9, 64)', 'k=(3072, 8, 9, 64)', 'v=(3072, 8, 9, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3072, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(3072, 9, 256)']\n",
      "inspect_shapes before attend: : ['q=(3072, 8, 9, 64)', 'k=(3072, 8, 9, 64)', 'v=(3072, 8, 9, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3072, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(3072, 9, 256)']\n",
      "inspect_shapes before attend: : ['q=(3072, 8, 9, 64)', 'k=(3072, 8, 9, 64)', 'v=(3072, 8, 9, 64)']\n",
      "inspect_shapes Transformer post attn: ['x=(3072, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(3072, 9, 256)']\n",
      "inspect_shapes MEGABYTE post proj: ['x=(3, 1024, 8, 256)']\n",
      "inspect_shapes MEGABYTE logits: ['logits=(3, 1024, 9, 256)']\n",
      "inspect_shapes MEGABYTE output logits: ['logits=(3, 8192, 256)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "_ = model(batch, encoder_hidden_states=encoder_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3f243f2-3cb7-4114-a5d1-4c11b7a21b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect_shapes MEGABYTE: ['ids=(4, 8192)']\n",
      "inspect_shapes Transformer: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post attn: ['x=(4, 1025, 512)']\n",
      "inspect_shapes Transformer post ff: ['x=(4, 1025, 512)']\n",
      "inspect_shapes MEGABYTE post proj: ['x=(4096, 8, 256)']\n",
      "inspect_shapes Transformer: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post attn: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post attn: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post attn: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post attn: ['x=(4096, 9, 256)']\n",
      "inspect_shapes Transformer post ff: ['x=(4096, 9, 256)']\n",
      "inspect_shapes MEGABYTE post proj: ['x=(4, 1024, 8, 256)']\n",
      "inspect_shapes MEGABYTE logits: ['logits=(4, 1024, 9, 256)']\n",
      "inspect_shapes MEGABYTE output logits: ['logits=(4, 8192, 256)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "_ = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ad2d3c-88b5-4a63-85bd-fbd3c5b1854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_properties = torch.cuda.get_device_properties(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a30392-edf8-40cc-9b6a-043e39996050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24152MB, multi_processor_count=82)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b235513a-0ee3-4391-a4b7-72590d700b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 12:33:27.065638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 12:33:27.940073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from omr.pipeline import OmrPipeline\n",
    "import os\n",
    "\n",
    "pipeline = OmrPipeline(\"StaffPad/MuseSight\", score_tokenizer_path=\"~\", instr_dir=\"~\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4565ef37-60f4-4020-acdc-59e59f38cd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1024, bias=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omr_model = pipeline.model\n",
    "omr_model.enc_to_dec_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3fd4779-1011-4d6d-8f77-255559970e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/home/gerald/Downloads/All The Things You Are.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4721b3bc-a579-45e4-a76f-bf6d32580411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1280, 960])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_proc = pipeline.image_processor\n",
    "\n",
    "x = img_proc(img, return_tensors='pt')\n",
    "x = x['pixel_values'].mean(axis=1, keepdim=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aace3b9e-07d9-4fdd-9c24-a53ef73820ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1201, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    Y = omr_model.encoder(x.cuda())\n",
    "    print(Y[0].shape)\n",
    "    Y = omr_model.enc_to_dec_proj(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b62421de-981a-47a7-82a0-8d4e95ec1778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1201, 1024])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "daf06ce3-6394-46eb-b864-99cbe4dabe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1280*960 / (32**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb968ce-3b3c-456e-9ff1-3b0e578ea00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omr",
   "language": "python",
   "name": "omr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
